---
name: Pull Request Validation

"on":
  pull_request:
    branches: [master, develop]
  # Manual trigger for testing workflow changes
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write
  checks: write
  statuses: write

env:
  DOTNET_VERSION: '10.0.x'
  # CRITICAL: Set STRICT_COVERAGE: true before merging to main
  # This enforces 70% minimum coverage threshold (lines 785, 810)
  # Current bypass is TEMPORARY for development only
  # TODO: Enable STRICT_COVERAGE when overall coverage ‚â• 70% (Sprint 2 milestone)
  #       Tracking: https://github.com/frigini/MeAjudaAi/issues/33
  #       References: docs/testing/code-coverage-guide.md#L297-L313
  # Re-enable when overall coverage reaches 70% (Sprint 2 milestone)
  STRICT_COVERAGE: false
  # PostgreSQL configuration (DRY principle - single source of truth)
  # Fallback credentials: Only used in fork/local dev; main repo requires secrets
  POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD || 'test123' }}
  POSTGRES_USER: ${{ secrets.POSTGRES_USER || 'postgres' }}
  POSTGRES_DB: ${{ secrets.POSTGRES_DB || 'meajudaai_test' }}

jobs:
  # Job 1: Code Quality Checks
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgis/postgis:16-3.4
        env:
          # Using workflow-level environment variables (defined at top)
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          POSTGRES_HOST_AUTH_METHOD: md5
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      azurite:
        image: mcr.microsoft.com/azure-storage/azurite:latest
        ports:
          - 10000:10000
          - 10001:10001
          - 10002:10002

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}

      - name: Validate Secrets Configuration
        run: |
          echo "Using PostgreSQL credentials with fallback defaults"
          echo "POSTGRES_USER: ${{ env.POSTGRES_USER }}"
          echo "POSTGRES_DB: ${{ env.POSTGRES_DB }}"
          echo "POSTGRES_PASSWORD: [REDACTED]"

          echo "Database configuration validated"

      - name: Check Keycloak Configuration
        env:
          KEYCLOAK_ADMIN_PASSWORD: ${{ secrets.KEYCLOAK_ADMIN_PASSWORD }}
        run: |
          echo "üîç Checking Keycloak configuration..."
          if [ -z "$KEYCLOAK_ADMIN_PASSWORD" ]; then
            echo "‚ÑπÔ∏è  KEYCLOAK_ADMIN_PASSWORD secret not configured - Keycloak is optional"
            echo "üí° To enable Keycloak authentication features, configure the secret in:"
            echo "   Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí KEYCLOAK_ADMIN_PASSWORD"
            echo "üîÑ Tests will continue without Keycloak-dependent features"
          else
            echo "‚úÖ Keycloak secrets configured - authentication features enabled"
          fi

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: üîß Restore dependencies
        run: dotnet restore MeAjudaAi.sln --force-evaluate

      - name: Build solution
        run: dotnet build MeAjudaAi.sln --configuration Release --no-restore

      - name: Wait for PostgreSQL to be ready
        env:
          PGPASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
        run: |
          echo "üîÑ Waiting for PostgreSQL to be ready..."
          echo "Debug: POSTGRES_USER=$POSTGRES_USER"
          echo "Debug: Checking PostgreSQL availability..."

          counter=1
          max_attempts=60

          while [ $counter -le $max_attempts ]; do
            if pg_isready -h localhost -p 5432 -U "$POSTGRES_USER"; then
              echo "‚úÖ PostgreSQL is ready!"
              break
            fi
            echo "Waiting for PostgreSQL... ($counter/$max_attempts)"
            sleep 3
            counter=$((counter + 1))
          done

          # Check if we exited the loop due to timeout
          if ! pg_isready -h localhost -p 5432 -U "$POSTGRES_USER"; then
            echo "‚ùå PostgreSQL failed to become ready within 180 seconds"
            echo "Debug: Checking PostgreSQL logs..."
            echo "Service container id: ${{ job.services.postgres.id }}"
            docker logs "${{ job.services.postgres.id }}" || echo "Could not get PostgreSQL logs"
            exit 1
          fi

      - name: Setup PostgreSQL connection
        id: db
        uses: ./.github/actions/setup-postgres-connection
        with:
          postgres-host: localhost
          postgres-port: 5432
          postgres-db: ${{ env.POSTGRES_DB }}
          postgres-user: ${{ env.POSTGRES_USER }}
          postgres-password: ${{ env.POSTGRES_PASSWORD }}

      - name: Run tests with coverage
        env:
          ASPNETCORE_ENVIRONMENT: Testing
          # Pre-built connection string (optional, takes precedence if available)
          DB_CONNECTION_STRING: ${{ secrets.DB_CONNECTION_STRING }}
          # PostgreSQL connection for CI
          EXTERNAL_POSTGRES_HOST: localhost
          EXTERNAL_POSTGRES_PORT: 5432
          MEAJUDAAI_DB_HOST: localhost
          MEAJUDAAI_DB_PORT: 5432
          MEAJUDAAI_DB_PASS: ${{ env.POSTGRES_PASSWORD }}
          MEAJUDAAI_DB_USER: ${{ env.POSTGRES_USER }}
          MEAJUDAAI_DB: ${{ env.POSTGRES_DB }}
          # Legacy environment variables for compatibility
          DB_HOST: localhost
          DB_PORT: 5432
          DB_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          DB_USERNAME: ${{ env.POSTGRES_USER }}
          DB_NAME: ${{ env.POSTGRES_DB }}
          # Keycloak settings
          KEYCLOAK_ADMIN_PASSWORD: ${{ secrets.KEYCLOAK_ADMIN_PASSWORD }}
          # Azure Storage (Azurite emulator)
          # ‚ö†Ô∏è TEST CREDENTIALS ONLY - These are the standard Azurite local emulator credentials
          # These are intentionally public and documented at:
          # https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azurite#well-known-storage-account-and-key
          AZURE_STORAGE_CONNECTION_STRING: "DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://localhost:10000/devstoreaccount1;"
          AzureStorage__ConnectionString: "DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://localhost:10000/devstoreaccount1;"
          # Map connection strings to .NET configuration using double underscore
          ConnectionStrings__DefaultConnection: ${{ steps.db.outputs.connection-string }}
          ConnectionStrings__Users: ${{ steps.db.outputs.connection-string }}
          ConnectionStrings__Search: ${{ steps.db.outputs.connection-string }}
          ConnectionStrings__meajudaai-db: ${{ steps.db.outputs.connection-string }}
        run: |
          set -euo pipefail
          echo "üß™ Executando testes com cobertura consolidada..."

          # Function to escape single quotes in PostgreSQL connection string values
          escape_single_quotes() {
            echo "$1" | sed "s/'/''/g"
          }

          # Build .NET connection string from PostgreSQL secrets with proper quoting
          # Check if a pre-built connection string secret exists first
          if [ -n "${DB_CONNECTION_STRING:-}" ]; then
            export ConnectionStrings__DefaultConnection="$DB_CONNECTION_STRING"
            echo "‚úÖ Using pre-built connection string from DB_CONNECTION_STRING secret"
          else
            # Build connection string with proper Npgsql quoting for special characters
            ESCAPED_DB=$(escape_single_quotes "$MEAJUDAAI_DB")
            ESCAPED_USER=$(escape_single_quotes "$MEAJUDAAI_DB_USER")
            ESCAPED_PASS=$(escape_single_quotes "$MEAJUDAAI_DB_PASS")

            DB_CONN_STR="Host=localhost;Port=5432;Database='$ESCAPED_DB'"
            DB_CONN_STR="${DB_CONN_STR};Username='$ESCAPED_USER';Password='$ESCAPED_PASS'"
            export ConnectionStrings__DefaultConnection="$DB_CONN_STR"
            echo "‚úÖ Built connection string from PostgreSQL secrets with proper quoting"
          fi

          # Test database connection first
          echo "Testing database connection..."
          PGPASSWORD="$MEAJUDAAI_DB_PASS" \
            psql -h localhost \
                 -U "$MEAJUDAAI_DB_USER" \
                 -d "$MEAJUDAAI_DB" \
                 -c "SELECT 1;" || {
            echo "‚ùå Database connection failed ‚Äî aborting workflow"
            echo "üí° Ensure PostgreSQL service is running and secrets are configured:"
            echo "   - POSTGRES_PASSWORD, POSTGRES_USER, POSTGRES_DB"
            echo "üîÑ Integration tests require database connectivity to validate changes"
            exit 1
          }

          # Remove any existing coverage data
          rm -rf ./coverage
          mkdir -p ./coverage

          echo "üß™ Running unit tests with coverage for all modules..."

          # Define modules for coverage testing
          # STRATEGY: Comprehensive coverage for Domain, Application, and critical Shared/ApiService layers
          # - Domain/Application: Core business logic (80-100% target)
          # - Shared/ApiService: Reusable infrastructure, middlewares, extensions (60-80% target)
          # - Excluded: Integration tests, E2E tests, Architecture tests (tested separately below)
          # 
          # RATIONALE FOR INFRASTRUCTURE EXCLUSION:
          # Infrastructure tests are excluded from module-based coverage runs (--filter) because:
          # 1. Infrastructure tests often require real dependencies (databases, message brokers, etc.)
          # 2. Mixing unit tests (mocked) with infrastructure tests distorts coverage metrics
          # 3. Infrastructure/Integration tests run separately (lines 345-366) with proper test environment
          # 4. This separation ensures unit test coverage accurately reflects business logic coverage
          # 
          # FORMAT: "ModuleName:path/to/module/tests/:IncludeFilter"
          MODULES=(
            # Domain module tests
            "Users:src/Modules/Users/Tests/:MeAjudaAi.Modules.Users.*"
            "Providers:src/Modules/Providers/Tests/:MeAjudaAi.Modules.Providers.*"
            "Documents:src/Modules/Documents/Tests/:MeAjudaAi.Modules.Documents.*"
            "ServiceCatalogs:src/Modules/ServiceCatalogs/Tests/:MeAjudaAi.Modules.ServiceCatalogs.*"
            "Locations:src/Modules/Locations/Tests/:MeAjudaAi.Modules.Locations.*"
            "SearchProviders:src/Modules/SearchProviders/Tests/:MeAjudaAi.Modules.SearchProviders.*"

            # System/Shared tests (626+ tests - HIGH PRIORITY)
            "Shared:tests/MeAjudaAi.Shared.Tests/:MeAjudaAi.Shared"
            "ApiService:tests/MeAjudaAi.ApiService.Tests/:MeAjudaAi.ApiService"
          )

          # Run unit tests for each module with coverage
          for module_info in "${MODULES[@]}"; do
            IFS=':' read -r module_name module_path include_pattern <<< "$module_info"

            if [ -d "$module_path" ]; then
              echo "Running $module_name tests with coverage..."

              # Create specific output directory for this module
              MODULE_COVERAGE_DIR="./coverage/${module_name,,}"
              mkdir -p "$MODULE_COVERAGE_DIR"

            # Dynamic include filter based on module type
            if [ -n "$include_pattern" ]; then
              INCLUDE_FILTER="[${include_pattern}]*"
            else
              INCLUDE_FILTER="[MeAjudaAi.*]*"
            fi

            # Validate filter is not empty or trivial
            if [ "$INCLUDE_FILTER" = "[]*" ] || [ "$INCLUDE_FILTER" = "[].*" ]; then
              echo "‚ö†Ô∏è  INCLUDE_FILTER is trivial, falling back to [MeAjudaAi.*]*"
              INCLUDE_FILTER="[MeAjudaAi.*]*"
            fi

            # NOTE: EXCLUDE_FILTER includes compiler-generated code exclusions
            # to provide accurate coverage metrics for hand-written code only.
            # Excluded patterns:
            #   - Test assemblies: [*.Tests*]*, [*Test*]*, [testhost]*
            #   - OpenApi generated: [*Microsoft.AspNetCore.OpenApi.Generated*]*
            #   - Compiler services: [*System.Runtime.CompilerServices*]*
            #   - Regex generator: [*System.Text.RegularExpressions.Generated*]*
            EXCLUDE_FILTER="[*.Tests*]*,[*Test*]*,[testhost]*,[*Microsoft.AspNetCore.OpenApi.Generated*]*,[*System.Runtime.CompilerServices*]*,[*System.Text.RegularExpressions.Generated*]*"

            echo "  Include: $INCLUDE_FILTER"
            echo "  Exclude: $EXCLUDE_FILTER"

            # Create temporary runsettings file for this module
            cat > "/tmp/${module_name,,}.runsettings" <<EOF
<?xml version="1.0" encoding="utf-8"?>
<RunSettings>
  <DataCollectionRunSettings>
    <DataCollectors>
      <DataCollector friendlyName="XPlat Code Coverage">
        <Configuration>
          <Format>opencover</Format>
          <Include>${INCLUDE_FILTER}</Include>
          <Exclude>${EXCLUDE_FILTER}</Exclude>
        </Configuration>
      </DataCollector>
    </DataCollectors>
  </DataCollectionRunSettings>
</RunSettings>
EOF

            dotnet test "$module_path" \
                --configuration Release \
                --verbosity normal \
                --filter "FullyQualifiedName!~Integration&FullyQualifiedName!~Infrastructure" \
                --collect:"XPlat Code Coverage" \
                --results-directory "$MODULE_COVERAGE_DIR" \
                --logger "trx;LogFileName=${module_name,,}-test-results.trx" \
                --settings "/tmp/${module_name,,}.runsettings"

              # Find and rename the coverage file to a predictable name
              if [ -d "$MODULE_COVERAGE_DIR" ]; then
                echo "üîç Searching for coverage files in $MODULE_COVERAGE_DIR..."
                echo "üìÇ Directory contents:"
                find "$MODULE_COVERAGE_DIR" -type f -name "*.xml" | head -10
                
                # Look for both opencover and cobertura formats (search recursively in GUID subdirs)
                COVERAGE_FILE=$(find "$MODULE_COVERAGE_DIR" -type f \
                  \( -name "coverage.opencover.xml" -o -name "coverage.cobertura.xml" \) \
                  -print -quit)
                if [ -f "$COVERAGE_FILE" ]; then
                  echo "‚úÖ Found coverage file: $COVERAGE_FILE"
                  # Copy to standardized name based on original format
                  if [[ "$COVERAGE_FILE" == *"cobertura"* ]]; then
                    cp "$COVERAGE_FILE" "$MODULE_COVERAGE_DIR/${module_name,,}.cobertura.xml"
                  else
                    cp "$COVERAGE_FILE" "$MODULE_COVERAGE_DIR/${module_name,,}.opencover.xml"
                  fi
                else
                  echo "‚ö†Ô∏è  Coverage file not found for $module_name"
                  echo "üìã Available XML files:"
                  find "$MODULE_COVERAGE_DIR" -name "*.xml" -type f | head -5
                fi
              else
                echo "‚ùå Coverage directory not found: $MODULE_COVERAGE_DIR"
              fi
            else
              echo "‚ö†Ô∏è  $module_name tests not found at $module_path - skipping"
            fi
          done

          echo ""
          echo "üìä COVERAGE FILES SUMMARY"
          echo "========================="
          echo "Total coverage XML files generated:"
          find ./coverage -type f \( -name "*.opencover.xml" -o -name "*.cobertura.xml" \) | wc -l
          echo ""
          echo "Coverage files by module:"
          find ./coverage -type f \( -name "*.opencover.xml" -o -name "*.cobertura.xml" \) -printf "%f\n" | sort | head -20
          echo ""

          echo "üß™ Running system tests without coverage collection..."

          # Define system tests (no coverage) - architectural and integration validation only
          SYSTEM_TESTS=(
            "Architecture:tests/MeAjudaAi.Architecture.Tests/"
            "Integration:tests/MeAjudaAi.Integration.Tests/"
            "E2E:tests/MeAjudaAi.E2E.Tests/"
          )

          # Run system tests without coverage
          for test_info in "${SYSTEM_TESTS[@]}"; do
            IFS=':' read -r test_name test_path <<< "$test_info"

            if [ -d "$test_path" ]; then
              echo "Running $test_name tests (no coverage)..."
              dotnet test "$test_path" \
                --configuration Release \
                --no-build \
                --verbosity normal \
                --logger "trx;LogFileName=${test_name,,}-test-results.trx"
            else
              echo "‚ö†Ô∏è  $test_name tests not found at $test_path - skipping"
            fi
          done

          echo "‚úÖ Todos os testes executados com sucesso"

      - name: CRITICAL - Hangfire Npgsql 10.x Compatibility Tests
        env:
          ASPNETCORE_ENVIRONMENT: Testing
          EXTERNAL_POSTGRES_HOST: localhost
          EXTERNAL_POSTGRES_PORT: 5432
          MEAJUDAAI_DB_HOST: localhost
          MEAJUDAAI_DB_PORT: 5432
          MEAJUDAAI_DB_PASS: ${{ env.POSTGRES_PASSWORD }}
          MEAJUDAAI_DB_USER: ${{ env.POSTGRES_USER }}
          MEAJUDAAI_DB: ${{ env.POSTGRES_DB }}
          ConnectionStrings__DefaultConnection: ${{ steps.db.outputs.connection-string }}
          ConnectionStrings__HangfireConnection: ${{ steps.db.outputs.connection-string }}
        run: |
          set -euo pipefail
          echo "üö® CRITICAL: Running Hangfire + Npgsql 10.x compatibility tests"
          echo "=========================================================================="
          echo "CONTEXT:"
          echo "  - Hangfire.PostgreSql 1.20.12 compiled against Npgsql 6.x"
          echo "  - Npgsql 10.x introduces BREAKING CHANGES"
          echo "  - These tests VALIDATE runtime compatibility"
          echo "  - DEPLOYMENT IS BLOCKED if these tests fail"
          echo "=========================================================================="
          echo ""

          # Run ONLY Hangfire integration tests with explicit filter
          TEST_EXIT_CODE=0
          dotnet test tests/MeAjudaAi.Integration.Tests/ \
            --configuration Release \
            --no-build \
            --verbosity normal \
            --filter "Category=HangfireIntegration" \
            --logger "console;verbosity=detailed" \
            --logger "trx;LogFileName=hangfire-integration-test-results.trx" \
            || TEST_EXIT_CODE=$?

          echo ""
          if [ $TEST_EXIT_CODE -eq 0 ]; then
            echo "‚úÖ HANGFIRE COMPATIBILITY VALIDATED"
            echo "   - All Hangfire integration tests passed"
            echo "   - Hangfire.PostgreSql 1.20.12 is compatible with Npgsql 10.x"
            echo "   - Safe to deploy to production"
          else
            echo "‚ùå HANGFIRE COMPATIBILITY TEST FAILED"
            echo "=========================================================================="
            echo "CRITICAL FAILURE: Hangfire.PostgreSql 1.20.12 is NOT compatible with Npgsql 10.x"
            echo ""
            echo "REQUIRED ACTIONS:"
            echo "  1. DO NOT MERGE this PR"
            echo "  2. DO NOT DEPLOY to production"
            echo "  3. Review test failures in the logs above"
            echo "  4. Choose one mitigation option:"
            echo ""
            echo "MITIGATION OPTIONS:"
            echo "  A. DOWNGRADE (Recommended for immediate fix):"
            echo "     - Downgrade to EF Core 9.x + Npgsql 8.x"
            echo "     - Update Directory.Packages.props:"
            echo "       <PackageVersion Include=\"Npgsql.EntityFrameworkCore.PostgreSQL\" Version=\"9.0.0\" />"
            echo "       <PackageVersion Include=\"Npgsql\" Version=\"8.0.5\" />"
            echo ""
            echo "  B. WAIT FOR UPDATE:"
            echo "     - Monitor: https://github.com/frankhommers/Hangfire.PostgreSql"
            echo "     - Wait for Hangfire.PostgreSql 2.x with Npgsql 10 support"
            echo ""
            echo "  C. ALTERNATIVE BACKEND:"
            echo "     - Switch to Hangfire.Pro.Redis (requires license)"
            echo "     - Or use Hangfire.InMemory for testing only"
            echo ""
            echo "üìñ See: docs/deployment_environments.md for rollback procedures"
            echo "=========================================================================="
            exit $TEST_EXIT_CODE
          fi

      - name: Validate namespace reorganization
        run: |
          echo "üîç Validating namespace reorganization..."
          if grep -R -nE '^[[:space:]]*using[[:space:]]+MeAjudaAi\.Shared\.Common;' -- src/ \
              2>/dev/null; then
            echo "‚ùå Found old namespace imports"
            exit 1
          else
            echo "‚úÖ Conformidade com namespaces validada"
          fi

      - name: Upload coverage reports
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: coverage-reports
          path: coverage/**
          if-no-files-found: ignore

      - name: Upload Test Results
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: test-results
          path: "**/*.trx"
          if-no-files-found: ignore

      - name: List Coverage Files (Debug)
        run: |
          echo "üîç Listing coverage files for debugging..."
          echo "Coverage directory structure:"
          find ./coverage -type f 2>/dev/null | head -20 || echo "No files found in coverage directory"
          echo ""
          echo "OpenCover XML files:"
          find ./coverage -name "*.opencover.xml" -type f 2>/dev/null || echo "No .opencover.xml files found"
          echo ""
          echo "Any XML files:"
          find ./coverage -name "*.xml" -type f 2>/dev/null || echo "No XML files found"
          echo ""
          echo "Coverage directory contents:"
          ls -la ./coverage/ 2>/dev/null || echo "Coverage directory not found"
          echo ""
          echo "Checking for coverage.xml files:"
          find ./coverage -name "coverage.xml" -type f 2>/dev/null || echo "No coverage.xml files found"

      - name: Fix Coverage Files (if needed)
        run: |
          echo "üîß Attempting to fix coverage file locations and names..."

          # Find any coverage.xml files and rename them to appropriate format
          find ./coverage -name "coverage.xml" -type f | while read -r file; do
            dir=$(dirname "$file")
            module=$(basename "$dir")
            new_file="$dir/$module.cobertura.xml"
            echo "Copying $file to $new_file"
            cp "$file" "$new_file"
          done

          # Find coverage files in nested directories and copy to module directories
          find ./coverage -type f \
            \( -name "coverage.opencover.xml" -o -name "coverage.cobertura.xml" \) \
            | while read -r file; do
            # Get the module directory (should be like ./coverage/users/)
            module_dir=$(echo "$file" | sed 's|coverage/\([^/]*\)/.*|coverage/\1|')
            module_name=$(basename "$module_dir")

            # Determine target file based on source format
            if [[ "$file" == *"cobertura"* ]]; then
              target_file="$module_dir/$module_name.cobertura.xml"
            else
              target_file="$module_dir/$module_name.opencover.xml"
            fi

            if [ "$file" != "$target_file" ]; then
              echo "Copying $file to $target_file"
              cp "$file" "$target_file" 2>/dev/null || true
            fi
          done

          echo "Coverage files after processing:"
          find ./coverage -name "*.xml" -type f 2>/dev/null || echo "No XML coverage files found"

      - name: Code Coverage Summary
        id: coverage_opencover
        continue-on-error: true
        uses: irongut/CodeCoverageSummary@v1.3.0
        with:
          filename: 'coverage/**/*.opencover.xml'
          badge: true
          fail_below_min: false
          format: markdown
          hide_branch_rate: false
          hide_complexity: false
          indicators: true
          output: both
          thresholds: '70 85'

      - name: Alternative Coverage Summary (Cobertura format)
        id: coverage_cobertura
        if: ${{ always() && steps.coverage_opencover.outcome != 'success' }}
        uses: irongut/CodeCoverageSummary@v1.3.0
        with:
          filename: 'coverage/**/*.cobertura.xml'
          badge: true
          fail_below_min: false
          format: markdown
          hide_branch_rate: false
          hide_complexity: false
          indicators: true
          output: both
          thresholds: '70 85'
        continue-on-error: true

      - name: Fallback Coverage Summary (any XML)
        id: coverage_fallback
        if: >-
          ${{ always() &&
              steps.coverage_opencover.outcome != 'success' &&
              steps.coverage_cobertura.outcome != 'success' }}
        uses: irongut/CodeCoverageSummary@v1.3.0
        with:
          filename: 'coverage/**/*.xml'
          badge: true
          fail_below_min: false
          format: markdown
          hide_branch_rate: false
          hide_complexity: false
          indicators: true
          output: both
          thresholds: '70 85'
        continue-on-error: true

      - name: Display Coverage Percentages
        if: always()
        run: |
          echo "üìä CODE COVERAGE SUMMARY"
          echo "========================"
          echo ""
          # Look for coverage files and extract basic statistics
          for coverage_file in $(find ./coverage -name "*.opencover.xml" -o -name "*.xml" | head -5); do
            if [ -f "$coverage_file" ]; then
              echo "üìÑ Coverage file: $coverage_file"
              # Extract line coverage using grep/awk if available
              if command -v awk >/dev/null 2>&1; then
                # Try to extract coverage statistics from OpenCover XML
                COVERAGE_LINE_ATTR='sequenceCoverage="[^"]*"'
                COVERAGE_BRANCH_ATTR='branchCoverage="[^"]*"'
                lines_covered=$(grep -o "$COVERAGE_LINE_ATTR" "$coverage_file" 2>/dev/null | \
                  head -1 | grep -o '[0-9.]*' || echo "N/A")
                branch_covered=$(grep -o "$COVERAGE_BRANCH_ATTR" "$coverage_file" 2>/dev/null | \
                  head -1 | grep -o '[0-9.]*' || echo "N/A")
                if [ "$lines_covered" != "N/A" ]; then
                  echo "  üìà Line Coverage: ${lines_covered}%"
                fi
                if [ "$branch_covered" != "N/A" ]; then
                  echo "  üåø Branch Coverage: ${branch_covered}%"
                fi
              fi
              echo ""
            fi
          done

          echo "üí° For detailed coverage report, check the 'Code Coverage Summary' step above"
          echo "üéØ Minimum thresholds: 70% (warning) / 85% (good)"

      - name: Select Coverage Outputs
        id: select_coverage_outputs
        if: always()
        run: |
          echo "üéØ Selecting coverage outputs from available steps..."

          # First try to use step outputs if available
          if [ "${{ steps.coverage_opencover.outcome }}" = "success" ] &&
             [ -n "${{ steps.coverage_opencover.outputs.summary }}" ]; then
            SUMMARY="${{ steps.coverage_opencover.outputs.summary }}"
            BADGE="${{ steps.coverage_opencover.outputs.badge }}"
            SOURCE="OpenCover"
            echo "Using OpenCover coverage outputs"
          elif [ "${{ steps.coverage_cobertura.outcome }}" = "success" ] &&
               [ -n "${{ steps.coverage_cobertura.outputs.summary }}" ]; then
            SUMMARY="${{ steps.coverage_cobertura.outputs.summary }}"
            BADGE="${{ steps.coverage_cobertura.outputs.badge }}"
            SOURCE="Cobertura"
            echo "Using Cobertura coverage outputs"
          elif [ "${{ steps.coverage_fallback.outcome }}" = "success" ] &&
               [ -n "${{ steps.coverage_fallback.outputs.summary }}" ]; then
            SUMMARY="${{ steps.coverage_fallback.outputs.summary }}"
            BADGE="${{ steps.coverage_fallback.outputs.badge }}"
            SOURCE="Fallback XML"
            echo "Using Fallback coverage outputs"
          else
            # Fallback: Check if coverage files exist and generate basic summary
            echo "Step outputs not available, checking for coverage files..."

            if find coverage -name "*.opencover.xml" -type f | head -1 >/dev/null 2>&1; then
              COVERAGE_FILE=$(find coverage -name "*.opencover.xml" -type f | head -1)
              SOURCE="OpenCover (Direct)"
              echo "Found OpenCover file: $COVERAGE_FILE"
            elif find coverage -name "*.cobertura.xml" -type f | head -1 >/dev/null 2>&1; then
              COVERAGE_FILE=$(find coverage -name "*.cobertura.xml" -type f | head -1)
              SOURCE="Cobertura (Direct)"
              echo "Found Cobertura file: $COVERAGE_FILE"
            elif find coverage -name "*.xml" -type f | head -1 >/dev/null 2>&1; then
              COVERAGE_FILE=$(find coverage -name "*.xml" -type f | head -1)
              SOURCE="XML (Direct)"
              echo "Found XML file: $COVERAGE_FILE"
            else
              COVERAGE_FILE=""
              SOURCE="None"
              echo "No coverage files found"
            fi

            if [ -n "$COVERAGE_FILE" ]; then
              # Try to extract basic coverage percentage from XML
              if command -v grep >/dev/null 2>&1; then
                # Look for line-rate or sequenceCoverage attributes
                LINE_RATE=$(grep -o 'line-rate="[^"]*"' "$COVERAGE_FILE" 2>/dev/null | head -1 | cut -d'"' -f2)
                if [ -z "$LINE_RATE" ]; then
                  LINE_RATE=$(grep -o 'sequenceCoverage="[^"]*"' "$COVERAGE_FILE" 2>/dev/null | head -1 | cut -d'"' -f2)
                fi

                if [ -n "$LINE_RATE" ]; then
                  # Convert decimal to percentage if needed
                  if [ "$(echo "$LINE_RATE" | cut -d'.' -f1)" = "0" ]; then
                    PERCENTAGE=$(echo "$LINE_RATE * 100" | bc -l 2>/dev/null || \
                                echo "scale=1; $LINE_RATE * 100" | bc 2>/dev/null || \
                                echo "Unknown")
                  else
                    PERCENTAGE="$LINE_RATE"
                  fi
                  SUMMARY="**Coverage**: ${PERCENTAGE}% (extracted from $SOURCE)"
                  BADGE="![Coverage](https://img.shields.io/badge/coverage-${PERCENTAGE}%25-brightgreen)"
                else
                  SUMMARY="**Coverage**: Available (file found, percentage not extracted)"
                  BADGE="![Coverage](https://img.shields.io/badge/coverage-available-blue)"
                fi
              else
                SUMMARY="**Coverage**: Files found but could not extract percentage"
                BADGE="![Coverage](https://img.shields.io/badge/coverage-found-blue)"
              fi
            else
              SUMMARY="Coverage data not available"
              BADGE=""
            fi
          fi

          # Export outputs
          echo "source=$SOURCE" >> $GITHUB_OUTPUT
          echo "badge=$BADGE" >> $GITHUB_OUTPUT

          # Export multiline summary using heredoc
          {
            echo 'summary<<EOF'
            echo "$SUMMARY"
            echo 'EOF'
          } >> $GITHUB_OUTPUT

          echo "Coverage source: $SOURCE"
          echo "Summary: $SUMMARY"

      - name: Add Coverage PR Comment
        uses: marocchino/sticky-pull-request-comment@v2
        if: github.event_name == 'pull_request'
        with:
          recreate: true
          header: coverage-report
          message: |
            ## üìä Code Coverage Report

            ${{ steps.select_coverage_outputs.outputs.summary }}

            ### üìà Coverage Details
            - **Coverage badges**: ${{ steps.select_coverage_outputs.outputs.badge }}
            - **Minimum threshold**: 70% (warning) / 85% (good)
            - **Report format**: Auto-detected from OpenCover/Cobertura XML files
            - **Coverage source**: ${{ steps.select_coverage_outputs.outputs.source }}

            ### üìã Coverage Analysis
            - **Line Coverage**: Shows percentage of code lines executed during tests
            - **Branch Coverage**: Shows percentage of code branches/conditions tested
            - **Complexity**: Code complexity metrics for maintainability

            ### üéØ Quality Gates
            - ‚úÖ **Pass**: Coverage ‚â• 85%
            - ‚ö†Ô∏è **Warning**: Coverage 70-84%
            - ‚ùå **Fail**: Coverage < 70%

            ### üìÅ Artifacts
            - **Coverage reports**: Available in workflow artifacts
            - **Test results**: TRX files with detailed test execution data

            *This comment is updated automatically on each push to track coverage trends.*

      # Refactored: Extracted to reusable action for better maintainability
      # See: .github/actions/validate-coverage/README.md for documentation
      - name: Validate Coverage Thresholds
        if: always()
        uses: ./.github/actions/validate-coverage
        with:
          coverage-directory: './coverage'
          threshold: '70'
          strict-mode: ${{ env.STRICT_COVERAGE }}
          opencover-output: ${{ steps.coverage_opencover.outputs.line-rate }}
          opencover-outcome: ${{ steps.coverage_opencover.outcome }}
          cobertura-output: ${{ steps.coverage_cobertura.outputs.line-rate }}
          cobertura-outcome: ${{ steps.coverage_cobertura.outcome }}
          fallback-output: ${{ steps.coverage_fallback.outputs.line-rate }}
          fallback-outcome: ${{ steps.coverage_fallback.outcome }}

  # Job 2: Security Scan (Consolidated)
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}

      - name: Restore dependencies
        run: dotnet restore MeAjudaAi.sln --force-evaluate

      - name: Run Security Audit
        run: dotnet list package --vulnerable --include-transitive

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: OSV-Scanner (fail on HIGH/CRITICAL)
        run: |
          echo "üîç Installing OSV-Scanner..."
          # Install OSV-Scanner with pinned version for reproducibility
          OSV_VERSION="v1.8.3"
          OSV_URL="https://github.com/google/osv-scanner/releases/download/${OSV_VERSION}"

          # Retry logic for network transient failures
          max_retries=3
          retry_count=0

          while [ $retry_count -lt $max_retries ]; do
            if curl -sSfL --connect-timeout 10 --max-time 30 \
                "${OSV_URL}/osv-scanner_linux_amd64" -o osv-scanner; then
              echo "‚úÖ Download successful"
              chmod +x osv-scanner
              break
            else
              retry_count=$((retry_count + 1))
              if [ $retry_count -lt $max_retries ]; then
                echo "‚ö†Ô∏è  Download failed (attempt $retry_count/$max_retries), retrying in 5s..."
                sleep 5
              else
                echo "‚ùå Download failed after $max_retries attempts"
                echo "üí° Trying fallback: install via Go toolchain..."

                # Fallback: Install via Go if available
                if command -v go >/dev/null 2>&1; then
                  echo "‚ÑπÔ∏è  Using Go to install OSV-Scanner..."
                  go install github.com/google/osv-scanner/cmd/osv-scanner@${OSV_VERSION}
                  # Copy to local dir for consistent usage
                  cp "$(go env GOPATH)/bin/osv-scanner" ./osv-scanner 2>/dev/null || \
                    ln -s "$(go env GOPATH)/bin/osv-scanner" ./osv-scanner
                else
                  echo "‚ùå Go toolchain not available, cannot install OSV-Scanner"
                  echo "‚ö†Ô∏è  CRITICAL: Vulnerability scan cannot be performed"
                  echo "üí° MITIGATION: Ensure runner has Go ‚â•1.18 installed or provide pre-built OSV-Scanner binary"
                  echo "üí° SECURITY NOTE: This workflow MUST fail when vulnerability scanning is unavailable."
                  echo "   To fix: ensure Go is installed on the runner or configure a pre-built OSV-Scanner binary."
                  exit 1  # Fail the workflow - security scanning is mandatory
                fi
              fi
            fi
          done

          echo "OSV-Scanner version and help:"
          ./osv-scanner --version || echo "Version command failed"
          ./osv-scanner scan --help | head -20 || echo "Help command failed"

          echo "üîç Running vulnerability scan..."
          # Run OSV-Scanner and capture exit code
          osv_exit_code=0
          ./osv-scanner scan --recursive --skip-git . || osv_exit_code=$?

          if [ $osv_exit_code -eq 0 ]; then
            echo "‚úÖ No vulnerabilities found"
          else
            echo "‚ö†Ô∏è OSV-Scanner found vulnerabilities (exit code: $osv_exit_code)"
            echo "üìÑ Running detailed scan for analysis..."

            # Run again with JSON output for detailed analysis
            ./osv-scanner scan --recursive --skip-git --format json . > osv-results.json || true

            if [ -f osv-results.json ]; then
              # Count HIGH/CRITICAL by CVSS (>=7.0) with safe parsing and textual severity mapping
              HIGH_OR_CRIT=$(
                jq -r '
                  # Function to safely convert severity scores to numbers
                  def safe_score_to_number:
                    if type == "number" then .
                    elif type == "string" then
                      (tonumber? // (
                        (. | ascii_upcase) as $upper
                        | if $upper == "CRITICAL" then 9
                          elif $upper == "HIGH" then 7
                          elif $upper == "MEDIUM" then 5
                          elif $upper == "LOW" then 3
                          else 0
                          end
                      ))
                    else 0
                    end;

                  [.results[]?.packages[]?.vulnerabilities[]?
                    | ( [(.severity // [])[]?.score? | safe_score_to_number] | max // 0 )
                    | select(. >= 7.0)
                  ] | length
                ' osv-results.json 2>/dev/null
              )

              if [ "${HIGH_OR_CRIT:-0}" -gt 0 ]; then
                echo "‚ùå Found $HIGH_OR_CRIT HIGH/CRITICAL vulnerabilities"
                echo "üìã First 10 vulnerabilities found:"
                # Use inline jq program to display vulnerability summary
                jq -r '.results[]?.packages[]?.vulnerabilities[]? |
                  "- \(.id): \(.summary // "No summary")"' \
                  osv-results.json 2>/dev/null | head -10
                echo ""
                echo "üö´ Security scan failed - vulnerabilities detected"
                echo "üí° Please review and fix vulnerabilities before merging"

                # Fail the workflow
                exit 1
              else
                echo "‚úÖ No HIGH/CRITICAL vulnerabilities found"
                # Count total vulnerabilities for info
                TOTAL_VULNS=$(jq -r '.results[]?.packages[]?.vulnerabilities[]? | .id' \
                  osv-results.json 2>/dev/null | wc -l)
                if [ "$TOTAL_VULNS" -gt 0 ]; then
                  echo "‚ÑπÔ∏è  Found $TOTAL_VULNS low/medium severity vulnerabilities (ignored)"
                fi
              fi
            else
              echo "‚ùå OSV-Scanner failed but no results file generated"
              exit 1
            fi
          fi

      - name: Secret Detection with TruffleHog
        if: ${{ github.event_name == 'pull_request' }}
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: ${{ github.event.pull_request.base.ref }}
          head: ${{ github.event.pull_request.head.sha }}
          extra_args: --debug --only-verified
  # Job 3: Markdown Link Validation (Simplified)
  markdown-link-check:
    name: Validate Markdown Links
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Cache lychee results
        uses: actions/cache@v4
        with:
          path: .lycheecache
          key: lychee-${{ runner.os }}-${{ hashFiles('**/*.md','config/lychee.toml') }}
          restore-keys: |
            lychee-${{ runner.os }}-

      - name: Check markdown links with lychee
        uses: lycheeverse/lychee-action@v1.10.0
        with:
          # Use simplified configuration for reliability
          args: >-
            --config config/lychee.toml
            --no-progress
            --cache
            --max-cache-age 1d
            "docs/**/*.md"
            "README.md"
          # Don't fail the entire pipeline on link check failures
          fail: false
          jobSummary: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Report link check results
        if: always()
        run: |
          echo "üìã Link validation completed (non-blocking)"
          echo "‚ÑπÔ∏è  Check job summary for detailed results"

  # Job 4: Simple YAML Validation (Quiet)
  yaml-validation:
    name: YAML Syntax Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install yamllint
        run: python3 -m pip install yamllint

      - name: Validate workflow files only
        run: |
          echo "üîç Validating critical YAML files..."
          if ! python3 -m yamllint -c .yamllint.yml .github/workflows/; then
            echo "‚ùå YAML validation failed"
            echo "‚ÑπÔ∏è  Check yamllint output above for details"
            exit 1
          fi
          echo "‚úÖ YAML validation completed"

